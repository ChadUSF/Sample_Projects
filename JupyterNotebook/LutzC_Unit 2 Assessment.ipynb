{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will focus on airline incidents. The data set for this assignment includes information on the cost of bird strikes. Use this data set to see if you can predict the cost of a bird strike (i.e., the `Total Cost` column in the data set) based on the attributes of the incident. This is important because this model can make a cost prediction as soon as a bird strike incident happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Variables\n",
    "\n",
    "The description of variables are provided in \"Airline - Data Dictionary.docx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Use the **airline.csv** data set and build models to predict **Total Cost**.\n",
    "\n",
    "**Be careful: this is a REGRESSION task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:\n",
    "\n",
    "Please save and submit this Jupyter notebook file. The correctness of the code matters for your grade. **Readability and organization of your code is also important.** You may lose points for submitting unreadable/undecipherable code. Therefore, use markdown cells to create sections, and use comments where necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hints:\n",
    "\n",
    "* This assignment requires you to work with a text-based column in addition to regular numeric/categorical columns. So you will have to pay attention to your pipelines during data processing.\n",
    "* You can do your data prep before or after the train/test split. Regardless, you should use train_test_split only once. If you find yourself using it twice, it means you are doing something wrong.\n",
    "* Recommended approach: \n",
    "    * import the data and perform the train/test split - like we always do. \n",
    "    * identify the names of numeric, categorical, feature engineered, and text columns - like we always do\n",
    "    * create individual pipelines for each type of column - like we always do. For the text pipeline, I would recommend the TFIDF Vectorizer with SVDs. Though, you can also use TFIDF Vectorizer with top N terms (without SVDs).\n",
    "    * combine all pipelines using the column transformer - like we always do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset:\n",
    "\n",
    "airline = pd.read_csv(\"airline.csv\")\n",
    "airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(airline, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent variable for train and test\n",
    "train_y = train['Total Cost']\n",
    "test_y = test['Total Cost']\n",
    "\n",
    "# Create independent variables by removing the dependent variable for train and test\n",
    "train_inputs = train.drop(['Total Cost'], axis=1)\n",
    "test_inputs = test.drop(['Total Cost'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (1 point)\n",
    "\n",
    "Create one NEW feature from existing data. You either transform a single variable, or create a new variable from existing ones. \n",
    "\n",
    "Grading: \n",
    "- 0.5 points for creating the new feature correctly\n",
    "- 0.5 points for the justification of the new feature (i.e., why did you create this new feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the descriptive statistics for the 'Number Objects' column\n",
    "train_inputs['Number_Objects'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the value counts for 'Number Objects'\n",
    "train_inputs['Number_Objects'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to bin the 'Number Objects' data to help address skewed distribution\n",
    "\n",
    "def new_col(df):\n",
    "    #Create a copy so that we don't overwrite the existing dataframe\n",
    "    df2 = df.copy()\n",
    "    \n",
    "    df2['Number_Objects_binned'] = pd.cut(df2['Number_Objects'],\n",
    "                                       bins=[1,1.5,6,11,1000],\n",
    "                                       labels=False, \n",
    "                                       include_lowest=True,\n",
    "                                       ordered=True)\n",
    "    \n",
    "    return df2[['Number_Objects_binned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results of the new function on train_inputs\n",
    "new_col(train_inputs).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the text mining process\n",
    "\n",
    "def text_col(df):\n",
    "    #Create a copy so that we don't overwrite the existing dataframe\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    # First, convert the dataframe column to a numpy array. Then, call the ravel function to make it one-dimensional\n",
    "    return np.array(df1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the data types for processing consideration\n",
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()\n",
    "\n",
    "# Identify the binary, text and feature engineered columns so we can pass them through without transforming\n",
    "binary_columns = ['Warning']\n",
    "feat_eng_column = ['Number_Objects']\n",
    "text_column = ['Description']\n",
    "\n",
    "# Remove the binary and text column from categorical columns.\n",
    "categorical_columns.remove('Description')\n",
    "categorical_columns.remove('Warning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/Confirm binary columns\n",
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/Confirm numeric columns\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/Confirm categorical columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numeric pipeline\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical pipeline\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary pipeline with ordinal encoder to prevent object datatype with later transform\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=[['N', 'Y']]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binned values pipeline\n",
    "\n",
    "my_new_column = Pipeline(steps=[('my_new_column', FunctionTransformer(new_col)),\n",
    "                               ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text mining pipeline\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),            \n",
    "                ('new_column', FunctionTransformer(text_col)),\n",
    "                ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "                ('svd', TruncatedSVD(n_components=300, n_iter=10))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor with all the column types\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('binary', binary_transformer, binary_columns),\n",
    "        ('trans', my_new_column, feat_eng_column),\n",
    "        ('text', text_transformer, text_column)\n",
    "        ],\n",
    "        remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the transformed dataset\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Baseline (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for baseline determination\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "dummy_regr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Train RMSE\n",
    "dummy_train_pred = dummy_regr.predict(train_x)\n",
    "\n",
    "baseline_train_mse = mean_squared_error(train_y, dummy_train_pred)\n",
    "\n",
    "baseline_train_rmse = np.sqrt(baseline_train_mse)\n",
    "\n",
    "print('Baseline Train RMSE: {}' .format(baseline_train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Test RMSE\n",
    "dummy_test_pred = dummy_regr.predict(test_x)\n",
    "\n",
    "baseline_test_mse = mean_squared_error (test_y, dummy_test_pred)\n",
    "\n",
    "baseline_test_rmse = np.sqrt(baseline_test_mse)\n",
    "\n",
    "print('Baseline Test RMSE: {}' .format(baseline_test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: \n",
    "\n",
    "Build the following models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree: (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5) \n",
    "\n",
    "tree_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = tree_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = tree_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting? Provide your answer below. If yes, please add more cells below and show how you corrected overfitting. If your model is overfitting and you don't correct it, you will lose points. (0.25 points)\n",
    "\n",
    "Yes it is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import randomizedsearchCV function to perform a grid search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'min_samples_leaf': np.arange(1, 10), \n",
    "     'max_depth': np.arange(1,5)}\n",
    "  ]\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "grid_search = RandomizedSearchCV(tree_reg, param_grid, cv=5, n_iter=20,\n",
    "                           scoring='neg_mean_squared_error', verbose=1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = grid_search.best_estimator_.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = grid_search.best_estimator_.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting regressor (1 points):\n",
    "\n",
    "The voting regressor should have at least 3 individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more functions for Voting Regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Define the Voting Regressor parameters\n",
    "dtree_reg = DecisionTreeRegressor(max_depth=5)\n",
    "svm_reg = SVR(kernel=\"rbf\", C=1, epsilon=0.1, gamma=1) \n",
    "rnd_reg = RandomForestRegressor(n_estimators=500, max_depth=5, n_jobs=-1) \n",
    "\n",
    "voting_reg = VotingRegressor(\n",
    "            estimators=[('dt', dtree_reg), \n",
    "                        ('svr', svm_reg), \n",
    "                        ('rnd', rnd_reg)])\n",
    "\n",
    "voting_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = voting_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = voting_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect results each classifier\n",
    "for reg in (dtree_reg, svm_reg, rnd_reg, voting_reg):\n",
    "    reg.fit(train_x, train_y)\n",
    "    test_y_pred = reg.predict(test_x)\n",
    "    print(reg.__class__.__name__, 'Test rmse=', np.sqrt(mean_squared_error(test_y, test_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting? Provide your answer below. If yes, please add more cells below and show how you corrected overfitting. If your model is overfitting and you don't correct it, you will lose points. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the Voting Regressor parameters to address overfitting\n",
    "dtree_reg = DecisionTreeRegressor(max_depth=3)\n",
    "svm_reg = SVR(kernel=\"rbf\", C=20, epsilon=0.01, gamma=.01) \n",
    "rnd_reg = RandomForestRegressor(n_estimators=500, max_depth=3, n_jobs=-1) \n",
    "\n",
    "voting_reg = VotingRegressor(\n",
    "            estimators=[('dt', dtree_reg), \n",
    "                        ('svr', svm_reg), \n",
    "                        ('rnd', rnd_reg)])\n",
    "\n",
    "voting_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = voting_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = voting_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Boosting model: (1 point)\n",
    "\n",
    "Build either an Adaboost or a GradientBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor \n",
    "\n",
    "ada_reg = AdaBoostRegressor( \n",
    "            SVR(), n_estimators=500, \n",
    "            learning_rate=0.1) \n",
    "\n",
    "ada_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = ada_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = ada_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting? Provide your answer below. If yes, please add more cells below and show how you corrected overfitting. If your model is overfitting and you don't correct it, you will lose points. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to address overfitting by decreasing C from default 1 to 0.01 and decrease n_estimators\n",
    "ada_reg = AdaBoostRegressor( \n",
    "            SVR(C=0.01), n_estimators=200, \n",
    "            learning_rate=1) \n",
    "\n",
    "ada_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = ada_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = ada_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network: (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLPRegressor for NN\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,))\n",
    "\n",
    "mlp_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = mlp_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = mlp_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting? Provide your answer below. If yes, please add more cells below and show how you corrected overfitting. If your model is overfitting and you don't correct it, you will lose points. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,),\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True,\n",
    "                      alpha = 0.1)\n",
    "\n",
    "mlp_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE\n",
    "train_pred = mlp_reg.predict(train_x)\n",
    "\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Train RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = mlp_reg.predict(test_x)\n",
    "\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search (1 points)\n",
    "\n",
    "Perform either a full or randomized grid search on any model you want. There has to be at least two parameters for the search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,)],\n",
    "             'activation': ['relu', 'tanh', 'logistic'],\n",
    "             'alpha' : [0.00001, .0001, .001, .01]}\n",
    "\n",
    "mlp_gs = RandomizedSearchCV(MLPRegressor(max_iter=1000, early_stopping=True), \n",
    "                            param_grid, cv=3, \n",
    "                            random_state=13, \n",
    "                            n_iter=30, n_jobs=-1)\n",
    "\n",
    "mlp_gs.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RMSE:\n",
    "train_pred = mlp_gs.best_estimator_.predict(train_x)\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(train_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test RMSE\n",
    "test_pred = mlp_gs.best_estimator_.predict(test_x)\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print('Test RMSE: {}' .format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting? Provide your answer below. If yes, please add more cells below and show how you corrected overfitting. If your model is overfitting and you don't correct it, you will lose points. (0.25 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the least overfitting model yet. \n",
    "\n",
    "On a side note, don't listen to scikit-learn.org for MLPRegressor when it says, \"For small datasets, however, ‘lbfgs’ can converge faster and perform better.\" I used lbfgs and it just maxed out my CPU and spun until I interrupted the kernel. I guess their 'small' is smaller than 847x647. When I typed this note, I went back and tried decreasing the SVDs to 100 (from 300) and saw little impact on most of the test RSME values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion (3 points in total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the train and test values of each model you built (1 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All values are RMSE in dollars. OF is for OverFitt refined models.\n",
    "\n",
    "Baseline  Train: 566520, Test: 504074\n",
    "\n",
    "DT        Train: 254340, Test: 536621\n",
    "\n",
    "DT_GS OF  Train: 520785, Test: 528883\n",
    "\n",
    "Voting    Train: 332276, Test: 516355\n",
    "\n",
    "Voting OF Train: 375979, Test: 514490\n",
    "\n",
    "AdaBoost  Train: 575517, Test: 518163\n",
    "\n",
    "AdaBst OF Train: 576092, Test: 518926\n",
    "\n",
    "NN        Train: 577164, Test: 520376\n",
    "\n",
    "NN OF     Train: 577364, Test: 520604\n",
    "\n",
    "MLP_GS    Train: 577363, Test: 520603\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model performs the best and why? (1 points) \n",
    "\n",
    "Hint: The best model is the one that has the best TEST value (regardless of any of the training values). If you select your model based on TRAIN values, you will lose points."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "My refined Voting model resulted in the lowest Test RMSE with 514490 of all my models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it compare to baseline? (1 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unfortunately, none of my models beat the baseline. This seems to be because the dataset is small and there is little relationship between the input variables and the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
